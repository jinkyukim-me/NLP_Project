{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jungs\\Anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import krwordrank\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "from PIL import Image\n",
    "from pykospacing import spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 주소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'E:\\\\crawling_1sentence\\\\11.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리(.을 기준으로 한문장으로 인식후 자동 띄어쓰기 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(data_path):\n",
    "    file = open(data_path, 'r',encoding='UTF-8')\n",
    "    data =file.readlines()\n",
    "    data_list = []\n",
    "    for sentence in data:\n",
    "        list_sentence1 = sentence.replace('\\n','').replace('\"\"','').split('\\n')\n",
    "        for list_sentence2 in list_sentence1:\n",
    "            list_sentence3 = list_sentence2.replace('.','.  ..').split('  ..')\n",
    "            for list_sentence4 in list_sentence3:\n",
    "                list_sentence = list_sentence4.replace('?','?  ??').split('  ??')\n",
    "                for lines in list_sentence:\n",
    "                    line = spacing(lines).strip()\n",
    "                    data_list.append(line)\n",
    "    ex_list = list(set(data_list))\n",
    "    if '' in ex_list:\n",
    "        ex_list.remove('')\n",
    "    return ex_list\n",
    "texts = get_texts(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맛있는라떼한잔                                        ?오늘도어김없이출근하여뽑아먹는라떼한잔.\n",
      "이게그렇게맛있을수가없다.이풍미와바디감은정말훌륭하다.\n",
      "이낙으로하루를활기차게살아가는것같다.맛있는라떼한잔이미치는영향은정말대단하다.\n"
     ]
    }
   ],
   "source": [
    "kkma = Kkma()\n",
    "def get_texts1(data_path):\n",
    "    file = open(data_path, 'r',encoding='UTF-8')\n",
    "    data = file.read()\n",
    "    print(data)\n",
    "    data_list = []\n",
    "    sentences = kkma.sentences(data)\n",
    "#     for sentence in data:\n",
    "#         list_sentence1 = sentence.replace('\\n','').replace('\"\"','').split('\\n')\n",
    "#         for list_sentence2 in list_sentence1:\n",
    "#             list_sentence = list_sentence2.replace('\\n','').split('.')\n",
    "#             for lines in list_sentence:\n",
    "#                 line = spacing(lines).strip()\n",
    "#                 data_list.append(line)\n",
    "#     ex_list = list(set(data_list))\n",
    "#     if '' in ex_list:\n",
    "#         ex_list.remove('')\n",
    "    return sentences\n",
    "sentences = get_texts1(data_path)\n",
    "# texts = get_texts1(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘도 어김없이 출근하여 뽑아먹는 라떼 한 잔.', '이 낙으로 하루를 활기차게 살아가는 것 같다.', '맛있는 라떼 한 잔이 미치는 영향은 정말 대단하다.', '맛있는 라떼 한 잔 ?', '이게 그렇게 맛있을 수가 없다.', '이 풍미와 바디감은 정말 훌륭하다.']\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['맛있는 라 떼 한잔 ? 오늘도 어김없이 출근 하여 뽑아 먹는 라 떼 한잔. 이게 그렇게 맛있을 수가 없다.이', '풍미와 바디 감은 정말 훌륭하다.', '이 낙으로 하루를 활기차게 살아가는 것 같다.', '맛있는 라 떼 한잔이 미치는 영향은 정말 대단하다.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "krwordrank로 문장요약, 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘도 어김없이 출근하여 뽑아먹는 라떼 한 잔.\n",
      "['정말', '출근하여', '어김없이', '라떼', '활기차게']\n"
     ]
    }
   ],
   "source": [
    "# if txt_len > 800:\n",
    "#     num_keywords = 10\n",
    "# else:\n",
    "#     num_keywords = 5\n",
    "from krwordrank.sentence import summarize_with_sentences\n",
    "penalty = lambda x:0 if (8 <= len(x) <= 150) else 1\n",
    "stopwords = {'오늘'}\n",
    "keywords, sents = summarize_with_sentences(\n",
    "    texts,\n",
    "    penalty=penalty,\n",
    "    stopwords = stopwords,\n",
    "    diversity=0.5,\n",
    "    num_keywords=5,\n",
    "    num_keysents=1,\n",
    "    scaling=lambda x:1,\n",
    "    verbose=False,\n",
    "    min_count = 1\n",
    ")\n",
    "keyword = []\n",
    "\n",
    "for sent in sents:\n",
    "    print(sent)\n",
    "for i in keywords:\n",
    "    keyword.append(i)\n",
    "print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_wordcloud(data_path):\n",
    "        f = open(data_path, 'r',encoding='UTF8')\n",
    "        data = f.read()\n",
    "        \n",
    "        engine = Okt() \n",
    "        nouns1 = engine.nouns(data)\n",
    "        nouns1 = [n for n in nouns1 if len(n) > 1]\n",
    "\n",
    "        # Counter: 단어수 세기, 가장 많이 등장한 단어(명사) 40개\n",
    "        count = Counter(nouns1)\n",
    "        tags = count.most_common(100)\n",
    "        \n",
    "        # 워드 클라우드 배경 사진\n",
    "        #book_mask = np.array(Image.open('./img/1.png'))\n",
    "\n",
    "        # WordCloud, matplotlib: 단어 구름 그리기\n",
    "        font_path = 'C:\\\\Windows\\\\Fonts\\\\malgun.ttf'\n",
    "        wc = WordCloud(font_path=font_path, background_color='white', \n",
    "                       width=1000, height=1000, prefer_horizontal=0.8,\n",
    "                      font_step=10)\n",
    "        cloud = wc.generate_from_frequencies(dict(tags))\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_wordcloud(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
